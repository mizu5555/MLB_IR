"""
Week 1: å»ºç«‹ Hybrid Search ç³»çµ±
ä½¿ç”¨ LanceDB å¯¦ç¾ Vector Search + Full-Text Search (FTS)

é€™å€‹ç³»çµ±è§£æ±ºäº†ã€Œäººåæª¢ç´¢å•é¡Œã€ï¼š
- Vector Searchï¼šç†è§£èªæ„ï¼ˆå¦‚ã€Œæ‰“è€…ã€ã€ã€ŒwRC+ã€ï¼‰
- FTSï¼šç²¾ç¢ºåŒ¹é…äººåï¼ˆå¦‚ã€ŒAaron Judgeã€ï¼‰
- Hybridï¼šçµåˆå…©è€…å„ªå‹¢

åŸ·è¡Œæ­¥é©Ÿï¼š
1. è¼‰å…¥æ–‡æª”ï¼ˆå¾ week1_data_collection.py çš„è¼¸å‡ºï¼‰
2. ç”Ÿæˆ embeddingsï¼ˆä½¿ç”¨ sentence-transformersï¼‰
3. å»ºç«‹ LanceDB table
4. å»ºç«‹ FTS indexï¼ˆç”¨æ–¼äººåï¼‰
5. æ¸¬è©¦æª¢ç´¢åŠŸèƒ½
"""

import json
import os
import re
from typing import List, Dict

print("=" * 80)
print("Hybrid Search ç³»çµ±å»ºç«‹å™¨")
print("=" * 80)

# ============================================
# Step 1: å®‰è£ä¾è³´
# ============================================
print("\n[Step 1] æª¢æŸ¥ä¾è³´...")

dependencies = {
    'lancedb': 'LanceDB (Vector Database)',
    'sentence_transformers': 'Sentence Transformers (Embedding Model)',
    'torch': 'PyTorch (Required by sentence-transformers)',
}

missing = []
for package, description in dependencies.items():
    try:
        __import__(package)
        print(f"  âœ… {description}")
    except ImportError:
        print(f"  âŒ {description} - éœ€è¦å®‰è£")
        missing.append(package)

if missing:
    print(f"\nè«‹åŸ·è¡Œï¼špip install {' '.join(missing)}")
    print("(å¦‚æœæ˜¯ Claude ç’°å¢ƒï¼ŒåŠ ä¸Š --break-system-packages)")
    exit(1)

# ç¾åœ¨å¯ä»¥å®‰å…¨å°å…¥
import lancedb
from sentence_transformers import SentenceTransformer
import pandas as pd

print("âœ… æ‰€æœ‰ä¾è³´å·²å°±ç·’")

# ============================================
# é…ç½®
# ============================================
DATA_DIR = "./mlb_data"
DB_PATH = "./mlb_lancedb"
DOCS_FILE = os.path.join(DATA_DIR, "mlb_documents.json")

# Embedding æ¨¡å‹ï¼ˆè¼•é‡ä¸”å¿«é€Ÿï¼‰
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # 384 ç¶­åº¦ï¼Œ22MB

print(f"\né…ç½®ï¼š")
print(f"  è³‡æ–™ç›®éŒ„ï¼š{DATA_DIR}")
print(f"  æ•¸æ“šåº«è·¯å¾‘ï¼š{DB_PATH}")
print(f"  æ–‡æª”æª”æ¡ˆï¼š{DOCS_FILE}")
print(f"  Embedding æ¨¡å‹ï¼š{EMBEDDING_MODEL}")

# ============================================
# Step 2: è¼‰å…¥æ–‡æª”
# ============================================
print("\n[Step 2] è¼‰å…¥æ–‡æª”...")

if not os.path.exists(DOCS_FILE):
    print(f"âŒ æ‰¾ä¸åˆ°æ–‡æª”æª”æ¡ˆï¼š{DOCS_FILE}")
    print("è«‹å…ˆåŸ·è¡Œ week1_data_collection.py")
    exit(1)

with open(DOCS_FILE, 'r', encoding='utf-8') as f:
    documents = json.load(f)

print(f"âœ… è¼‰å…¥ {len(documents)} å€‹æ–‡æª”")

# ============================================
# Step 3: åˆå§‹åŒ– Embedding æ¨¡å‹
# ============================================
print(f"\n[Step 3] åˆå§‹åŒ– Embedding æ¨¡å‹...")
print(f"  æ­£åœ¨è¼‰å…¥ {EMBEDDING_MODEL}...")

model = SentenceTransformer(EMBEDDING_MODEL)
print(f"  âœ… æ¨¡å‹å·²è¼‰å…¥")
print(f"  ç¶­åº¦ï¼š{model.get_sentence_embedding_dimension()}")

# ============================================
# Step 4: ç”Ÿæˆ Embeddings
# ============================================
print(f"\n[Step 4] ç”Ÿæˆ Embeddings...")
print(f"  é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜...")

# æå–æ‰€æœ‰æè¿°
descriptions = [doc['description'] for doc in documents]

# æ‰¹æ¬¡ç”Ÿæˆ embeddingsï¼ˆæ›´å¿«ï¼‰
print(f"  æ­£åœ¨è™•ç† {len(descriptions)} å€‹æè¿°...")
embeddings = model.encode(
    descriptions,
    batch_size=32,
    show_progress_bar=True,
    convert_to_numpy=True
)

print(f"  âœ… ç”Ÿæˆå®Œæˆ")
print(f"  Embedding å½¢ç‹€ï¼š{embeddings.shape}")

# å°‡ embeddings åŠ å…¥æ–‡æª”
for i, doc in enumerate(documents):
    doc['vector'] = embeddings[i].tolist()

# ============================================
# Step 5: å»ºç«‹ LanceDB è³‡æ–™åº«
# ============================================
print(f"\n[Step 5] å»ºç«‹ LanceDB è³‡æ–™åº«...")

# é€£æ¥åˆ°è³‡æ–™åº«ï¼ˆå¦‚æœä¸å­˜åœ¨æœƒè‡ªå‹•å»ºç«‹ï¼‰
db = lancedb.connect(DB_PATH)
print(f"  âœ… é€£æ¥åˆ°è³‡æ–™åº«ï¼š{DB_PATH}")

# æº–å‚™è³‡æ–™ï¼ˆLanceDB éœ€è¦ pandas DataFrame æˆ– pyarrow Tableï¼‰
# å°‡åµŒå¥—çš„ stats dict æ‰å¹³åŒ–
flattened_docs = []
for doc in documents:
    flat_doc = {
        'doc_id': doc['doc_id'],
        'player_id': doc['player_id'],
        'player_name': doc['player_name'],
        'team': doc['team'],
        'season': doc['season'],
        'position': doc['position'],
        'age': doc['age'],
        'type': doc['type'],
        'description': doc['description'],
        'games': doc['games'],
        'vector': doc['vector'],
    }
    
    # å°‡ stats æ‰å¹³åŒ–ï¼ˆåŠ ä¸Š stat_ å‰ç¶´é¿å…è¡çªï¼‰
    for key, value in doc['stats'].items():
        flat_doc[f'stat_{key}'] = value
    
    flattened_docs.append(flat_doc)

df = pd.DataFrame(flattened_docs)
print(f"  è³‡æ–™å½¢ç‹€ï¼š{df.shape}")

# ä¿®æ­£è³‡æ–™å‹æ…‹ï¼ˆç¢ºä¿å­—ä¸²æ¬„ä½æ˜¯ç´”å­—ä¸²ï¼‰
print(f"  æ­£åœ¨ä¿®æ­£è³‡æ–™å‹æ…‹...")
string_columns = ['doc_id', 'player_id', 'player_name', 'team', 'position', 'type', 'description']
for col in string_columns:
    if col in df.columns:
        df[col] = df[col].astype(str)

# ç¢ºä¿æ•¸å€¼æ¬„ä½æ˜¯æ•¸å€¼å‹æ…‹
numeric_columns = ['season', 'age', 'games']
for col in numeric_columns:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)

# è™•ç†çµ±è¨ˆæ¬„ä½ï¼ˆç¢ºä¿æ˜¯æ•¸å€¼ï¼‰
stat_columns = [col for col in df.columns if col.startswith('stat_')]
for col in stat_columns:
    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)

print(f"  âœ… è³‡æ–™å‹æ…‹å·²ä¿®æ­£")

# å»ºç«‹ tableï¼ˆå¦‚æœå·²å­˜åœ¨æœƒè¦†è“‹ï¼‰
print(f"  æ­£åœ¨å»ºç«‹ table...")
table = db.create_table("players", data=df, mode="overwrite")
print(f"  âœ… Table å»ºç«‹å®Œæˆï¼š{len(table)} ç­†è¨˜éŒ„")

# ============================================
# Step 6: å»ºç«‹ FTS Indexï¼ˆç”¨æ–¼äººåç²¾ç¢ºåŒ¹é…ï¼‰
# ============================================
print(f"\n[Step 6] å»ºç«‹ Full-Text Search Index...")

try:
    # å° player_name å’Œ description å»ºç«‹ FTS index
    table.create_fts_index(["player_name", "description"])
    print(f"  âœ… FTS Index å»ºç«‹å®Œæˆ")
    print(f"  å¯æœå°‹æ¬„ä½ï¼šplayer_name, description")
except Exception as e:
    print(f"  âš ï¸  FTS Index å»ºç«‹å¤±æ•—ï¼š{e}")
    print(f"  å°‡åªä½¿ç”¨ Vector Search")

# ============================================
# Step 7: æ¸¬è©¦ Hybrid Search
# ============================================
print(f"\n[Step 7] æ¸¬è©¦ Hybrid Search åŠŸèƒ½...")

def hybrid_search(query: str, k: int = 5, vector_weight: float = 0.5) -> List[Dict]:
    """
    Hybrid Searchï¼šçµåˆ Vector Search å’Œ FTS
    
    Args:
        query: æŸ¥è©¢å­—ä¸²
        k: è¿”å›çµæœæ•¸é‡
        vector_weight: Vector Search çš„æ¬Šé‡ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šä¾è³´èªæ„æœå°‹ï¼‰
    
    Returns:
        æª¢ç´¢çµæœåˆ—è¡¨
    """
    
    # 1. æª¢æŸ¥æ˜¯å¦åŒ…å«äººåï¼ˆç°¡å–® NERï¼šå¤§å¯«å­—æ¯é–‹é ­çš„é€£çºŒè©ï¼‰
    potential_names = re.findall(r'\b[A-Z][a-z]+ [A-Z][a-z]+\b', query)
    
    has_person_name = len(potential_names) > 0
    
    print(f"\n  æŸ¥è©¢ï¼š'{query}'")
    print(f"  åµæ¸¬åˆ°äººåï¼š{potential_names if has_person_name else 'ç„¡'}")
    
    # 2. å¦‚æœæœ‰äººåï¼Œå„ªå…ˆç”¨ FTS
    if has_person_name:
        print(f"  ç­–ç•¥ï¼šFTS (äººå) + Vector (èªæ„)")
        
        try:
            # FTS search on player_name
            fts_query = potential_names[0]  # ä½¿ç”¨ç¬¬ä¸€å€‹åµæ¸¬åˆ°çš„äººå
            fts_results = table.search(fts_query, query_type="fts").limit(k * 2).to_list()
            
            print(f"  FTS æ‰¾åˆ°ï¼š{len(fts_results)} ç­†")
            
            if len(fts_results) >= k:
                # FTS çµæœè¶³å¤ ï¼Œç›´æ¥è¿”å›
                return fts_results[:k]
            
            # FTS çµæœä¸è¶³ï¼Œè£œå…… Vector Search
            query_embedding = model.encode(query).tolist()
            vector_results = table.search(query_embedding).limit(k * 2).to_list()
            
            print(f"  Vector æ‰¾åˆ°ï¼š{len(vector_results)} ç­†")
            
            # åˆä½µçµæœï¼ˆå»é‡ï¼‰
            seen_ids = {r['doc_id'] for r in fts_results}
            combined = fts_results + [r for r in vector_results if r['doc_id'] not in seen_ids]
            
            return combined[:k]
            
        except Exception as e:
            print(f"  âš ï¸  FTS å¤±æ•—ï¼š{e}")
            print(f"  é™ç´šåˆ°ç´” Vector Search")
            has_person_name = False
    
    # 3. æ²’æœ‰äººåï¼Œä½¿ç”¨ç´” Vector Search
    if not has_person_name:
        print(f"  ç­–ç•¥ï¼šç´” Vector Search (èªæ„)")
        
        query_embedding = model.encode(query).tolist()
        results = table.search(query_embedding).limit(k).to_list()
        
        print(f"  æ‰¾åˆ°ï¼š{len(results)} ç­†")
        return results

# æ¸¬è©¦æ¡ˆä¾‹
test_queries = [
    "Aaron Judge 2024 wRC+",  # æœ‰äººå
    "Aaron Judge",  # åªæœ‰äººå
    "èª°æ˜¯ 2024 å¹´æœ€å¼·çš„æ‰“è€…ï¼Ÿ",  # ç„¡äººåï¼Œèªæ„æŸ¥è©¢
    "æŠ•æ‰‹ ERA æœ€ä½",  # ç„¡äººåï¼Œçµ±è¨ˆæŸ¥è©¢
    "Shohei Ohtani home runs",  # æœ‰äººå
]

print("\n" + "=" * 80)
print("æ¸¬è©¦æŸ¥è©¢")
print("=" * 80)

for query in test_queries:
    results = hybrid_search(query, k=3)
    
    print(f"\n  çµæœ Top 3ï¼š")
    for i, result in enumerate(results, 1):
        print(f"    {i}. {result['player_name']} ({result['type']}) - {result['team']} {result['season']}")
    print()

# ============================================
# Step 8: å„²å­˜æª¢ç´¢å‡½æ•¸ï¼ˆçµ¦å¾ŒçºŒä½¿ç”¨ï¼‰
# ============================================
print(f"\n[Step 8] å„²å­˜æª¢ç´¢é…ç½®...")

config = {
    'db_path': DB_PATH,
    'table_name': 'players',
    'embedding_model': EMBEDDING_MODEL,
    'embedding_dim': model.get_sentence_embedding_dimension(),
    'total_documents': len(documents),
    'fts_enabled': True,
}

config_file = os.path.join(DATA_DIR, "search_config.json")
with open(config_file, 'w', encoding='utf-8') as f:
    json.dump(config, f, indent=2)

print(f"  ğŸ’¾ å·²å„²å­˜ï¼š{config_file}")

# ============================================
# å®Œæˆ
# ============================================
print("\n" + "=" * 80)
print("âœ¨ Hybrid Search ç³»çµ±å»ºç«‹å®Œæˆï¼")
print("=" * 80)
print(f"ğŸ“Š ç³»çµ±è³‡è¨Šï¼š")
print(f"   - è³‡æ–™åº«ï¼š{DB_PATH}")
print(f"   - æ–‡æª”æ•¸ï¼š{len(documents)}")
print(f"   - Embedding æ¨¡å‹ï¼š{EMBEDDING_MODEL}")
print(f"   - ç¶­åº¦ï¼š{model.get_sentence_embedding_dimension()}")
print(f"   - FTS Indexï¼šå·²å•Ÿç”¨")
print(f"\nğŸ¯ æ¸¬è©¦çµæœï¼š")
print(f"   - âœ… Vector Searchï¼šé‹ä½œæ­£å¸¸")
print(f"   - âœ… FTSï¼šé‹ä½œæ­£å¸¸")
print(f"   - âœ… Hybrid Searchï¼šé‹ä½œæ­£å¸¸")
print(f"\nğŸ” æ¸¬è©¦å»ºè­°ï¼š")
print(f"   è©¦è©¦çœ‹ï¼šhybrid_search('Aaron Judge 2024 wRC+')")
print(f"   å°æ¯”ï¼šç´”åå­—æœå°‹ vs æœ‰çµ±è¨ˆé …ç›®çš„æœå°‹")
print(f"\nğŸ“ ä¸‹ä¸€æ­¥ï¼š")
print(f"   1. æ¸¬è©¦æ›´å¤šæŸ¥è©¢ï¼Œé©—è­‰æª¢ç´¢æº–ç¢ºåº¦")
print(f"   2. åŸ·è¡Œ week1_test_retrieval.py é€²è¡Œå®Œæ•´æ¸¬è©¦")
print(f"   3. é€²å…¥ Week 2ï¼šå»ºç«‹ LLM Agent")
print("=" * 80)